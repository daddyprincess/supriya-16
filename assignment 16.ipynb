{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edbccb9-bce5-4d02-8b6b-5d6bb9213d89",
   "metadata": {},
   "source": [
    "## 1.webscrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7953b-fafa-4065-b902-835467dc5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web scraping is the process of collecting and parsing raw data from the Web, and the Python community has come up with some \n",
    "pretty powerful web scraping tools. The Internet hosts perhaps the greatest source of information on the planet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c986931-02bc-41c5-b0c3-45eb8934766a",
   "metadata": {},
   "source": [
    "### used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69a75d-bfc7-4fda-9267-289b7707e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python web scraping is an automated method used for collecting large amounts of data from websites and storing it in a\n",
    "structured form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97955d-8921-4baa-9ae5-3025244fb2cb",
   "metadata": {},
   "source": [
    "### three areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3051a-32b5-4be8-8281-cc808321491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "- data analysis and research\n",
    "- price monitoring and competitive analysis\n",
    "- content aggregation and news monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26ef20-3576-47d8-b943-45f75edfce54",
   "metadata": {},
   "source": [
    "## 2.different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0c9a6-c0ce-48bd-ad11-466e6e835923",
   "metadata": {},
   "source": [
    "### 1.requests and beautiful soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5b7a5-d8cb-416c-ba67-0825db989c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup is used extract information from the HTML and XML files. It provides a parse tree and the functions to navigate, search or modify this parse tree.\n",
    "\n",
    "-Beautiful Soup is a Python library used to pull the data out of HTML and XML files for web scraping purposes. It produces a parse tree from page source code that\n",
    " can be utilized to drag data hierarchically and more legibly. \n",
    "-It was first presented by Leonard Richardson, who is still donating to this project, and this project is also supported by Tide lift.\n",
    "-Beautiful soup3 was officially released in May 2006, Latest version released by Beautiful Soup is 4.9.2, and it supports Python 3 and Python 2.4 as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce7011-0618-4c02-b85a-5deb663bcf99",
   "metadata": {},
   "source": [
    "### 2.scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372ef00-6e6e-44b1-aeaa-4c3b4abc3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrapy is an open-source and collaborative framework for extracting the data a user needs from websites. Written in Python language, Scrapy is a fast high-level\n",
    "web crawling & scraping framework for Python. It can be used for a wide range of purposes, from data mining to monitoring and automated testing. It is basically\n",
    "an application framework for writing web spiders that crawl web sites and extract data from them. Spiders are the classes that a user defines and Scrapy uses the\n",
    "Spiders to scrape information from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d16d8-7f40-406f-9b7b-77a80e3e4fd1",
   "metadata": {},
   "source": [
    "### 3.pyquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffae865-7fd0-4a40-8b1c-190df4032452",
   "metadata": {},
   "outputs": [],
   "source": [
    "The lxml is a Python tool for C libraries libxml2 and libxslt. It is recognised as one of the feature-rich and easy-to-use libraries for processing XML and HTML \n",
    "in Python language. It is unique in the case that it combines the speed and XML feature of these libraries with the simplicity of a native Python API and is\n",
    "mostly compatible but superior to the well-known ElementTree_API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196e88a-b085-4c6f-86b7-61ef72b4262b",
   "metadata": {},
   "source": [
    "## 3.beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876b6c9-4d80-4a53-af2a-619a8dbaf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup is used extract information from the HTML and XML files. It provides a parse tree and the functions to navigate, search or modify this parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491814f8-e6dc-47da-ba17-5839d0809415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some real-world use cases of BeautifulSoup include\n",
    "\n",
    "-The Python bug tracker was transferred from Sourceforge to Roundup by the Python developers using Beautiful Soup.\n",
    "\n",
    "-Jiabao Lin's DXY-COVID-19-Crawler implements Beautiful Soup to scrape a Chinese medical website for data on COVID-19, making it easy for researchers to monitor\n",
    " the virus's transmission.\n",
    "\n",
    "-The NOAA's Forecast Applications Branch employs BeautifulSoup in the TopoGrabber script for downloading high-resolution USGS datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa6487-102c-4c60-8083-0de42fb29382",
   "metadata": {},
   "source": [
    "## 4.web scrapping project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21affb-843e-4909-b168-4aa433bbdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Primarily they use it to create powerful backend web applications which can handle data quickly. They can also employ it for activities as varied as data \n",
    "scraping, machine learning, and artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3ce4a-28e3-49fb-af7b-262d0e8c0b2f",
   "metadata": {},
   "source": [
    "## 5.names of AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ce475-817f-4c6f-b8ff-4d956f2c3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "These are several AWS services that can be used in a web scrapping project to enhance its fuctionality, scalability and realiability.Here are some AWS services \n",
    "commonly used in web scrapping project:\n",
    "    -Amazon EC2 (elastic compute cloud)\n",
    "    -Amazon s3 (simple storage service)\n",
    "    -Amazon DynamoDB\n",
    "    -AWS lambda\n",
    "    -Amazon cloud watch\n",
    "    -AWS step function\n",
    "    -AWS batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df1e9c-18ec-4537-95af-b67a5cb6c71b",
   "metadata": {},
   "source": [
    "### Amazon EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf2299-c9d9-4076-80d2-8c5d38b3235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Create a key pair and security group.\n",
    "-Select an Amazon Machine Image (AMI) and compatible instance type, then create an instance.\n",
    "-Stop and restart the instance.\n",
    "-Associate an Elastic IP address with your instance.\n",
    "-Connect to your instance with SSH, then clean up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89bb0a-40b9-4cad-ae0c-0875310433b8",
   "metadata": {},
   "source": [
    "### Amazon s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95974b82-4bbe-4f12-9918-ebdfca116978",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Setup an account. Right, let's start with creating your AWS account if you haven't already. ...\n",
    "Step 2: Create a user. ...\n",
    "Step 3: Create a bucket. ...\n",
    "Step 4: Create a policy and add it to your user. ...\n",
    "Step 5: Download AWS CLI and configure your user. ...\n",
    "Step 6: Upload your files. ...\n",
    "Step 7: Check if authentication is working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07e4ac-a871-4249-966e-9170586c2269",
   "metadata": {},
   "source": [
    "### Amazon DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426fc96-80ba-4100-8289-6d32aaf12aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Setup an account. Right, let's start with creating your AWS account if you haven't already. ...\n",
    "Step 2: Create a user. ...\n",
    "Step 3: Create a bucket. ...\n",
    "Step 4: Create a policy and add it to your user. ...\n",
    "Step 5: Download AWS CLI and configure your user. ...\n",
    "Step 6: Upload your files. ...\n",
    "Step 7: Check if authentication is working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85182730-4601-4191-a642-5fb58cdfa1a5",
   "metadata": {},
   "source": [
    "### AWS lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909f61f-62e8-4ebd-8cf8-0b45c8e53366",
   "metadata": {},
   "outputs": [],
   "source": [
    "-lambda is a serverless computing computing service that allows you to run code without provisioning or managing severs.\n",
    "-In a web scrapping project, you can use lambda functions to execute short-lived scraping tasks.\n",
    "-For example, you can trigger a lambda function to scrape website periodically or in response to specific events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ebd28-9fb9-457f-a06a-8076e8fdb275",
   "metadata": {},
   "source": [
    "### Amazon Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15bed1-2654-4be3-9c15-de21ea2f47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Glue is an extract, transform, load (ETL) service that can be used to prepare and transform the scraped data for analysis.\n",
    "-you can define data schemas, perform data transformations, and create ETL pipelines using Glue to clean, enrich, and structure the scraped data before storing \n",
    "or analysing  it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d50b3-6e8a-4b11-b633-1b06a06f1ab8",
   "metadata": {},
   "source": [
    "### AWS step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fb0e8-6f44-461f-baa6-4cd83b5d2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Step functions is a severless workflow service thet helps cordinate multiple AWS services and enables youmto build complex workflows for your web scraping\n",
    "project.\n",
    "-you can use step functions to orchestrate and scheduke scraping tasks, handle retries, and manage the flow of data between different steps or services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4d160-63f4-45a1-89cc-caed55b3250b",
   "metadata": {},
   "source": [
    "### AWS batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5a1ea-2385-44e2-95f0-aad6dc9024e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "-AWS batch is a fully manged service for running batch computing workloads.\n",
    "-It can be used to process and analyze the scraped data in a scalable and efficient manner,\n",
    "-You can create batch jobs to process the collected data, perform data transformations, and store the results in a database or data lake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
